# ðŸ”¥ Activation Functions in Deep Learning

This project explains **Activation Functions**, which are essential components in **Neural Networks** and **Deep Learning**. Activation functions are used to introduce **non-linearity** into the network, enabling it to learn complex patterns.

---

## ðŸ“Œ Table of Contents
1. [What is an Activation Function?]
2. [Types of Activation Functions]
   - [Sigmoid]
   - [Tanh]
   - [ReLU]
   - [Leaky ReLU]
   - [Softmax]

---

## ðŸ“Œ What is an Activation Function?

In **deep learning**, an **Activation Function** is a mathematical function that determines whether a neuron should be activated or not. It helps introduce **non-linearity** to the neural network, enabling it to **learn and represent complex relationships**.

---

## ðŸ“Œ Types of Activation Functions

Hereâ€™s a list of popular Activation Functions used in deep learning:

1.Sigmoid
2.Tanh (Hyperbolic Tangent)
3.ReLU (Rectified Linear Unit)
4.Leaky ReLU
5.Softmax
